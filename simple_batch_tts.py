#!/usr/bin/env python3
"""
ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒƒãƒTTSï¼šQwen3-TTSã®ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒãƒƒãƒå‡¦ç†ã®èƒ½åŠ›ã‚’æ¤œè¨¼

ã“ã®ã‚³ãƒ¼ãƒ‰ã¯:
- ãƒ†ã‚­ã‚¹ãƒˆã‚’å¥ç‚¹ã§åˆ†å‰²
- Qwen3-TTSã®ãƒãƒƒãƒå‡¦ç†ã§ä¸€æ‹¬ç”Ÿæˆ
- RTFã‚’è¨ˆæ¸¬

asyncio/Semaphoreãªã©ã®è¤‡é›‘ãªä¸¦åˆ—å‡¦ç†ã¯ä½¿ã‚ãšã€
å…¬å¼ã®ãƒãƒƒãƒå‡¦ç†æ©Ÿèƒ½ã ã‘ã§ã©ã“ã¾ã§é€Ÿã„ã‹æ¤œè¨¼ã™ã‚‹ã€‚
"""

import re
import time
from typing import List
import numpy as np
import torch
import soundfile as sf
from qwen_tts import Qwen3TTSModel


def split_text(text: str, max_chars: int = 50) -> List[str]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’å¥ç‚¹ä½ç½®ã§åˆ†å‰²
    
    Args:
        text: åˆ†å‰²ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        max_chars: 1ãƒãƒ£ãƒ³ã‚¯ã®ç›®å®‰æ–‡å­—æ•°
    
    Returns:
        åˆ†å‰²ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    # å¥ç‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ—¥æœ¬èªã¨è‹±èªã®å¥èª­ç‚¹ï¼‰
    sentence_end_pattern = r'[ã€‚ï¼ï¼Ÿ\.!?]'
    
    chunks = []
    current_chunk = ""
    
    # æ–‡å˜ä½ã§åˆ†å‰²
    sentences = re.split(f'({sentence_end_pattern})', text)
    
    # å¥èª­ç‚¹ã‚’å‰ã®æ–‡ã«çµåˆ
    merged_sentences = []
    for i in range(0, len(sentences), 2):
        if i + 1 < len(sentences):
            merged_sentences.append(sentences[i] + sentences[i + 1])
        elif sentences[i].strip():
            merged_sentences.append(sentences[i])
    
    # max_charså‰å¾Œã§ãƒãƒ£ãƒ³ã‚¯åŒ–
    for sentence in merged_sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ ã§ãã‚‹ã‹
        if len(current_chunk) + len(sentence) <= max_chars:
            current_chunk += sentence
        else:
            # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜ã—ã¦æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’é–‹å§‹
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence
    
    # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks


def main():
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
    sample_text = """
    ã“ã‚“ã«ã¡ã¯ã€‚ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚ç§ãŸã¡ã¯æ–°ã—ã„éŸ³å£°åˆæˆæŠ€è¡“ã‚’è©¦ã—ã¦ã„ã¾ã™ã€‚
    ã“ã®æŠ€è¡“ã§ã¯ã€é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å°ã•ãªéƒ¨åˆ†ã«åˆ†å‰²ã—ã¾ã™ã€‚ãã—ã¦ã€ãã‚Œãã‚Œã‚’ãƒãƒƒãƒå‡¦ç†ã™ã‚‹ã“ã¨ã§é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
    æœ€å¾Œã«ã€ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ã‚’é †ç•ªé€šã‚Šã«çµåˆã—ã¦å®Œæˆã•ã›ã¾ã™ã€‚ã“ã‚Œã¯éå¸¸ã«åŠ¹ç‡çš„ãªæ–¹æ³•ã§ã™ã€‚
    äººå·¥çŸ¥èƒ½ã®é€²æ­©ã«ã‚ˆã‚Šã€è‡ªç„¶ãªéŸ³å£°åˆæˆãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚ä»Šå¾Œã‚‚æŠ€è¡“ã¯é€²åŒ–ã—ç¶šã‘ã‚‹ã§ã—ã‚‡ã†ã€‚
    ã“ã®å®Ÿè¨¼ã‚³ãƒ¼ãƒ‰ãŒçš†æ§˜ã®ãŠå½¹ã«ç«‹ã¦ã‚Œã°å¹¸ã„ã§ã™ã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚
    """
    sample_text = sample_text.strip()
    
    print("=" * 70)
    print("ğŸš€ ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒƒãƒTTSæ¤œè¨¼ï¼ˆå…¬å¼ãƒãƒƒãƒå‡¦ç†ã®ã¿ï¼‰")
    print("=" * 70)
    
    # 1. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    print("\nğŸ”§ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    model_start = time.time()
    model = Qwen3TTSModel.from_pretrained(
        "Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice",
        device_map="cuda:0",
        dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
    )
    model_time = time.time() - model_start
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† ({model_time:.2f}ç§’)")
    
    # 2. ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
    print(f"\nğŸ“ ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ä¸­...")
    chunks = split_text(sample_text, max_chars=50)
    print(f"   å…¨ä½“: {len(sample_text)}æ–‡å­— -> {len(chunks)}ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")
    for i, chunk in enumerate(chunks):
        print(f"   [{i+1}] ({len(chunk):2d}æ–‡å­—) {chunk[:40]}...")
    
    # 3. ãƒãƒƒãƒéŸ³å£°ç”Ÿæˆï¼ˆå…¬å¼ã®ãƒãƒƒãƒå‡¦ç†ã®ã¿ï¼‰
    print(f"\nğŸµ ãƒãƒƒãƒéŸ³å£°ç”Ÿæˆä¸­ï¼ˆ{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸€æ‹¬å‡¦ç†ï¼‰...")
    generation_start = time.time()
    
    # ã“ã‚ŒãŒå…¬å¼ã®ãƒãƒƒãƒå‡¦ç†
    wavs, sr = model.generate_custom_voice(
        text=chunks,
        language=["Japanese"] * len(chunks),
        speaker=["Ono_Anna"] * len(chunks),
    )
    
    generation_time = time.time() - generation_start
    
    # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±è¡¨ç¤º
    print(f"\n   âœ… ãƒãƒƒãƒå‡¦ç†å®Œäº†:")
    chunk_durations = []
    for i, (chunk, wav) in enumerate(zip(chunks, wavs)):
        chunk_duration = len(wav) / sr
        chunk_durations.append(chunk_duration)
        print(f"      [{i+1}] {len(chunk):2d}æ–‡å­— -> {chunk_duration:5.2f}ç§’éŸ³å£°")
    
    # 4. éŸ³å£°çµåˆ
    print(f"\nğŸ”— éŸ³å£°çµåˆä¸­...")
    combine_start = time.time()
    combined_audio = np.concatenate(wavs)
    combine_time = time.time() - combine_start
    
    # 5. ä¿å­˜
    output_file = "simple_batch_output.wav"
    sf.write(output_file, combined_audio, sr)
    
    # 6. çµ±è¨ˆæƒ…å ±
    total_time = generation_time + combine_time
    audio_duration = len(combined_audio) / sr
    rtf = total_time / audio_duration
    
    print("\n" + "=" * 70)
    print("ğŸ“Š å‡¦ç†çµæœ")
    print("=" * 70)
    print(f"ç·æ–‡å­—æ•°:         {len(sample_text)} æ–‡å­—")
    print(f"ãƒãƒ£ãƒ³ã‚¯æ•°:       {len(chunks)}")
    print(f"éŸ³å£°ç”Ÿæˆæ™‚é–“:     {generation_time:.2f} ç§’")
    print(f"éŸ³å£°çµåˆæ™‚é–“:     {combine_time:.4f} ç§’")
    print(f"ç·å‡¦ç†æ™‚é–“:       {total_time:.2f} ç§’")
    print(f"ç”ŸæˆéŸ³å£°é•·:       {audio_duration:.2f} ç§’")
    print(f"RTF:              {rtf:.2f}")
    print(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ:     {len(sample_text) / total_time:.1f} æ–‡å­—/ç§’")
    print(f"\nå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:     {output_file}")
    print(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ: {sr} Hz")
    print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º:   {len(combined_audio) * 2 / 1024 / 1024:.2f} MB")
    
    print("\n" + "=" * 70)
    print("ğŸ’¡ ã“ã®ã‚³ãƒ¼ãƒ‰ã®ç‰¹å¾´:")
    print("   - asyncio/Semaphoreç­‰ã®è¤‡é›‘ãªä¸¦åˆ—å‡¦ç†ã¯ä¸ä½¿ç”¨")
    print("   - Qwen3-TTSã®å…¬å¼ãƒãƒƒãƒå‡¦ç†æ©Ÿèƒ½ã®ã¿ä½¿ç”¨")
    print("   - ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²â†’ãƒãƒƒãƒç”Ÿæˆâ†’çµåˆã®ã‚·ãƒ³ãƒ—ãƒ«ãªæµã‚Œ")
    print("   - RTF < 1.0 ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚ˆã‚Šé«˜é€ŸãªéŸ³å£°ç”Ÿæˆ")
    print("=" * 70)


if __name__ == "__main__":
    main()
