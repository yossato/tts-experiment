# 実験ログ

## 2026年2月14日 - プロジェクト開始

### 目標
長文テキストの高速音声合成において、メモリ効率と生成速度を最適化する。

---

## 実験1: 並列処理アプローチの検証

### 仮説
テキストを細かく分割して並列処理すれば、疑似的にRTFを改善できる。

### 試行1: Webサーバー + 並列リクエスト
- **アプローチ**: uvicorn 2ワーカー + Semaphore(2)
- **結果**: RTF 0.76、GPU利用率 ~60%
- **問題**: 
  - プロセス間の切り替えオーバーヘッド
  - リクエスト単位の処理でバッチ効率が低い

### 試行2: asyncio + Semaphore + 個別並列
- **アプローチ**: asyncioで複数チャンクを並列生成
- **結果**: GPU利用率 25-35%
- **問題**: 
  - 個別生成では GPU の並列化が活かせない
  - スレッドプールでも効果薄い

### 試行3: Qwen3-TTS ネイティブバッチ処理
- **アプローチ**: `model.generate_custom_voice(text=[リスト])`
- **結果**: ✅ **RTF 0.37、GPU利用率 ~100%**
- **発見**: 
  - ネイティブバッチ処理が圧倒的に高速
  - 並列制御の複雑さは不要だった

**結論**: Qwen3-TTS公式のバッチ処理だけで十分高速。

---

## 実験2: 長文対応とメモリ管理

### 問題発見
夏目漱石「吾輩は猫である」冒頭（493文字）でOOMエラー発生。

### 原因分析
- モデルは1つだけロード（メモリ使用OK）
- 全チャンクを一度にバッチ処理 → GPU メモリ不足
- RTX 3080 (10GB) では15-20チャンクが限界

### 解決策: 分割バッチ処理
```python
# 10チャンクずつバッチ処理
for i in range(0, total_chunks, 10):
    batch = chunks[i:i+10]
    wavs = model.generate_custom_voice(text=batch, ...)
```

### 結果
- ✅ OOMエラー解消
- ✅ RTF 0.42 を維持（ほぼ劣化なし）
- ✅ 任意の長さのテキストに対応可能

---

## 実験3: ストリーミング風再生の実装

### 動機
- 100秒の音声を40秒で生成できる（RTF 0.4）
- しかし全生成完了まで待つのは体感が悪い

### アプローチ: 自転車操業方式
1. **生成スレッド**: 10チャンクずつ生成してキューに投入
2. **再生スレッド**: キューから取得して順次再生
3. RTF < 1.0 なので、再生中に次のバッチを生成可能

### 実装のポイント
```python
# マルチスレッド + キュー
generation_thread = Thread(target=generate_worker)
playback_thread = Thread(target=playback_worker)
audio_queue = Queue()

# 順序保証
pending_audios = {}  # 順番待ちバッファ
while expected_chunk in pending_audios:
    play(pending_audios.pop(expected_chunk))
```

### 結果
- ✅ 91秒の音声でも17秒後には再生開始
- ✅ メモリ効率的
- ✅ 体感速度が大幅改善

---

## 実験4: Webサーバー版の実装

### 目的
ブラウザから簡単に使えるUI提供。

### 実装内容
- FastAPI + HTML/CSS/JavaScript
- シングルページアプリケーション
- REST API (`POST /api/tts`)
- 処理統計のリアルタイム表示

### 機能
- 複数話者選択（9種類）
- 3言語対応（日本語・英語・中国語）
- ブラウザで音声再生
- API経由での利用も可能

---

## 技術的発見まとめ

### 1. バッチ処理 > 並列処理
Qwen3-TTSのネイティブバッチ処理が、複雑な並列制御より遥かに効率的。

### 2. メモリ管理の重要性
RTX 3080 (10GB) では10-15チャンクが適切なバッチサイズ。

### 3. 体感速度の最適化
技術的な RTF だけでなく、「最初の音が聞こえるまでの時間」も重要。

### 4. ストリーミング機能の現状
- モデルレベルではサポート（97ms レイテンシー）
- Python API では未実装（シミュレートのみ）
- 真のストリーミングは DashScope API（商用）で利用可能

---

## 性能データ

### 短文（236文字）
| 方式 | チャンク数 | 生成時間 | 音声長 | RTF |
|------|-----------|---------|--------|-----|
| シンプルバッチ | 6 | 15.19s | 40.56s | 0.37 |
| Webサーバー | 4リクエスト | 18.11s | 23.68s | 0.76 |

### 長文（493文字）
| 方式 | チャンク数 | バッチ数 | 生成時間 | 音声長 | RTF |
|------|-----------|---------|---------|--------|-----|
| ストリーミング | 13 | 2 | 38.28s | 91.04s | 0.42 |

---

## 今後の課題

### 調査・検証
- [ ] バッチサイズの最適値（GPU別）
- [ ] 他の話者での性能比較
- [ ] 音声品質の定量評価（MOS、WER等）

### 実装改善
- [ ] バッチサイズの動的調整（GPU空きメモリに応じて）
- [ ] vLLM-Omni 統合（真のストリーミング）
- [ ] 複数GPU対応

### ユーザビリティ
- [ ] Web UI の改善（進捗バー、エラー処理）
- [ ] 設定ファイル対応（話者、バッチサイズ等）
- [ ] Docker コンテナ化

---

## 参考文献

- Qwen3-TTS Technical Report: https://arxiv.org/abs/2601.15621
- FlashAttention 2: https://arxiv.org/abs/2307.08691
- PyTorch CUDA Best Practices: https://pytorch.org/docs/stable/notes/cuda.html
